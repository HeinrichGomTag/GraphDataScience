{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOoBgMrtJBJf7Gz5f9M60UF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DRIVE_PATH = 'drive/MyDrive/Colab Notebooks/Intro_to_Transformers/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9QzALI9ZR00","executionInfo":{"status":"ok","timestamp":1687916575342,"user_tz":360,"elapsed":19514,"user":{"displayName":"Ilja Rausch","userId":"09224443308545937385"}},"outputId":"87413ca0-5f08-49dd-8845-63ff6bf48774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Install the transformers packages\n","!pip install transformers==4.28.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DeQcBJfw6RCq","executionInfo":{"status":"ok","timestamp":1687916600339,"user_tz":360,"elapsed":14164,"user":{"displayName":"Ilja Rausch","userId":"09224443308545937385"}},"outputId":"ad43fa34-d861-4f39-ef7e-2a63b2e22b97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.28.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkE2U_qa51hV"},"outputs":[],"source":["# Import the required modules:\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the GPT-2 model and tokenizer\n","model_name = 'gpt2'  # You can specify the model size (e.g., 'gpt2', 'gpt2-medium', 'gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)"]},{"cell_type":"code","source":["# Let's have a closer look at tokenization\n","ids = [t for t in range(0, 10)]\n","tokens = tokenizer.convert_ids_to_tokens(ids)\n","print(f'Ids: {ids}')\n","print(f'Tokens: {tokens}')"],"metadata":{"id":"726z2EEhcrxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_sequence = 'Sentence about solar energy.'\n","tokenizer.encode(input_sequence, return_tensors='pt')"],"metadata":{"id":"Qyy423sdbsr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens([31837,   594,   546,  6591,  2568,    13])"],"metadata":{"id":"T2FjM_p9b79X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the input, i.e. your 'prompt'.\n","input_sequence = 'Sentence about solar energy.'\n","# input_sequence = 'This is a nonsense sentence.'\n","# Encode the prompt\n","encoded_input = tokenizer.encode(input_sequence, return_tensors='pt')\n","\n","# Generate text using the trained GPT-2 model\n","\n","max_length = 50  # Maximum length of the generated text\n","num_return_sequences = 5  # Number of text sequences to generate\n","\n","# https://huggingface.co/transformers/v4.4.2/main_classes/model.html#generation\n","output_sequences = model.generate(\n","    input_ids=encoded_input,\n","    max_length=max_length,\n","    num_return_sequences=num_return_sequences,\n","    temperature=1.0,  # Controls the randomness of the generated text (higher values make it more random)\n","    repetition_penalty=1.0,  # Penalizes repeating words/phrases (higher values make it less likely to repeat)\n","    do_sample=True,\n","    top_k=50,  # Generates from the top k most likely next words\n","    top_p=0.95,  # Generates from the cumulative probability until it reaches the given probability p\n",")\n","\n","# Print the generated text\n","for i, sequence in enumerate(output_sequences):\n","    print('\\n')\n","    # Decode the generated text\n","    generated_text = tokenizer.decode(sequence, skip_special_tokens=True)\n","    print(f\"Generated Text {i+1}: {generated_text}\\n\")"],"metadata":{"id":"64eYSSh66a5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","Image(DRIVE_PATH+'GPT_training_pipeline.png') # Reference: https://youtu.be/bZQun8Y4L2A?t=76 (retrieved 06-04-2023)"],"metadata":{"id":"Aqdo5v21o6Bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the GPT2 transformer"],"metadata":{"id":"KCM06uOvn9qY"}},{"cell_type":"markdown","source":["## Load a pretrained model"],"metadata":{"id":"EIu7qxw7tJt9"}},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n","from transformers import TextDataset, DataCollatorForLanguageModeling # https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/language_modeling.py\n","from transformers import Trainer, TrainingArguments\n","\n","model_name = 'gpt2'  # You can specify the model size (e.g., 'gpt2', 'gpt2-medium', 'gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)"],"metadata":{"id":"7h5jrtQkn6re"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the dataset"],"metadata":{"id":"vq7CfqyatNgc"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_file_path = DRIVE_PATH+'positive_statements.txt'\n","\n","# Load the text dataset\n","dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=train_file_path,\n","    block_size=32  # Adjust the block size based on your dataset and memory capacity\n",")\n","\n","# Split into train, eval and test sets\n","train_set, test_set = train_test_split(dataset, test_size=0.33, random_state=42)\n","eval_set, test_set = train_test_split(test_set, test_size=0.1, random_state=42)"],"metadata":{"id":"n-SZl7mTmMNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune the model on your dataset"],"metadata":{"id":"wmcFwiVytVCE"}},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","# Create a data collator for language modeling\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False\n",")\n","\n","output_dir = 'trained_models'  # Create the directory if it doesn't exist\n","\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,  # Specify the number of training epochs\n","    per_device_train_batch_size=64,  # Adjust the batch size based on your memory capacity\n","    save_steps=1000,\n","    save_total_limit=2,\n","    logging_steps=100,\n","    learning_rate=1e-4,  # Adjust the learning rate as needed\n",")\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_set,\n","    eval_dataset=eval_set,\n",")\n","\n","# Start the fine-tuning process\n","trainer.train()\n","\n","elapsed_time = time.time()-start_time\n","print(f'Elapsed time: {elapsed_time}')"],"metadata":{"id":"7wQwSC1E6paj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model('trained_models/final_finetuned_gpt2')"],"metadata":{"id":"mMOLeRtcriBN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the fine-tuned model"],"metadata":{"id":"T9tKTWI0tEn4"}},{"cell_type":"code","source":["model_name = 'trained_models/final_finetuned_gpt2'\n","finetuned_model = GPT2LMHeadModel.from_pretrained(model_name, local_files_only=True)"],"metadata":{"id":"occ7SiTEqo5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Sentence about solar energy.\"\n","# text = \"This is a nonsense sentence.\"\n","# text = \"A goat went crazy and tripped on mushrooms.\"\n","text = \"The analyst shared stock market insights with the Financial Times.\"\n","\n","# Tokenize the input text\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","\n","# Generate text using the trained GPT-2 model\n","max_length = 50  # Maximum length of the generated text\n","num_return_sequences = 10  # Number of text sequences to generate\n","\n","# https://huggingface.co/transformers/v4.4.2/main_classes/model.html#generation\n","output_sequences = finetuned_model.generate(\n","    input_ids=input_ids,\n","    max_length=max_length,\n","    num_return_sequences=num_return_sequences,\n","    temperature=1.0,  # Controls the randomness of the generated text (higher values make it more random)\n","    repetition_penalty=1.0,  # Penalizes repeating words/phrases (higher values make it less likely to repeat)\n","    do_sample=True,\n","    top_k=50,  # Generates from the top k most likely next words\n","    top_p=0.95,  # Generates from the cumulative probability until it reaches the given probability p\n",")\n","\n","# Decode and print the generated text\n","for output in output_sequences:\n","  print('\\n')\n","  # Decode the generated text\n","  generated_text = tokenizer.decode(output, skip_special_tokens=True)\n","  print(generated_text)"],"metadata":{"id":"9DApF7LNM6Me"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine tuning into a Classifier\n","GPT is a gnerative AI model, not designed for Classification tasks.\n","However, there are many other models out there. Here, we will use ROBERTa.\n","ROBERTa: https://huggingface.co/transformers/v4.4.2/model_doc/roberta.html\n","\n","Fine tuning tutorial: https://huggingface.co/transformers/v3.2.0/custom_datasets.html"],"metadata":{"id":"wCil-6CWe0zZ"}},{"cell_type":"markdown","source":["## Load and prepare the dataset"],"metadata":{"id":"hUNy0_IxkWDW"}},{"cell_type":"code","source":["positive_text = ''\n","\n","with open(DRIVE_PATH+'positive_statements.txt', 'r') as pos_file:\n","  positive_text = pos_file.read()\n","\n","positive_seqs = positive_text.split('\\n')\n","positive_labels = [1]*len(positive_seqs)"],"metadata":{"id":"TblYUt48fUhP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negative_text = ''\n","\n","with open(DRIVE_PATH+'negative_statements.txt', 'r') as neg_file:\n","  negative_text = neg_file.read()\n","\n","negative_seqs = negative_text.split('\\n')\n","negative_labels = [0]*len(negative_seqs)"],"metadata":{"id":"hGecchWbf8VY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","texts = np.array(positive_seqs + negative_seqs)\n","texts = np.array([text.replace(':', '') for text in texts])\n","labels = np.array(positive_labels + negative_labels)\n","\n","# Select only a small part of the data to reduce training time!\n","ids_to_select = np.array(range(0, len(labels)))\n","np.random.shuffle(ids_to_select)\n","ids_to_select = ids_to_select[:400]\n","texts = texts[ids_to_select]\n","labels = labels[ids_to_select]"],"metadata":{"id":"r7j-MH1lgQqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    texts, labels, test_size=0.33, random_state=35858)\n","\n","eval_texts, test_texts, eval_labels, test_labels = train_test_split(\n","    test_texts, test_labels, test_size=0.1, random_state=234)"],"metadata":{"id":"aBop8vI7gz0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0, 10):\n","  sentiment = 'positive' if train_labels[i] else 'negative'\n","  print(f'Sentence: {train_texts[i]} Sentiment: {sentiment}')"],"metadata":{"id":"76BcEVjfiZ6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SolarDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"VB9yJXJjmnjE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize using ROBERTa tokenizer"],"metadata":{"id":"RBVN2TfZkZuw"}},{"cell_type":"code","source":["from transformers import SqueezeBertTokenizerFast\n","tokenizer = SqueezeBertTokenizerFast.from_pretrained('squeezebert/squeezebert-uncased')\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# Compute the tokens\n","train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors='pt')\n","eval_encodings = tokenizer(list(eval_texts), truncation=True, padding=True, return_tensors='pt')\n","test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, return_tensors='pt')\n","\n","# Create the datasets\n","train_dataset = SolarDataset(train_encodings, train_labels)\n","eval_dataset = SolarDataset(eval_encodings, eval_labels)\n","test_dataset = SolarDataset(test_encodings, test_labels)"],"metadata":{"id":"3IpA4LV4gY63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load a pretrained ROBERTa model"],"metadata":{"id":"6MI1-498kg5o"}},{"cell_type":"code","source":["from transformers import SqueezeBertForSequenceClassification, Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='./classification_results',          # output directory\n","    num_train_epochs=1,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    logging_dir='./classification_logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","\n","# Following the 🤗 recommendation for the choice of the pretrained model:\n","# https://huggingface.co/transformers/v4.4.2/model_doc/squeezebert.html#overview\n","classifier_model = SqueezeBertForSequenceClassification.from_pretrained(\"squeezebert/squeezebert-mnli-headless\")\n","\n","classifier_trainer = Trainer(\n","    model=classifier_model,              # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset            # evaluation dataset\n",")\n","\n","classifier_trainer.train()"],"metadata":{"id":"Nc1DqVbSi1vO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_trainer.save_model('trained_models/final_finetuned_squeezebert_classifier')"],"metadata":{"id":"UQWR0dFwlShu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'trained_models/final_finetuned_squeezebert_classifier'\n","classifier_model = SqueezeBertForSequenceClassification.from_pretrained(model_name, local_files_only=True)\n","classifier_model.eval()"],"metadata":{"id":"sc4KbmbXow-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forward pass\n","with torch.no_grad():\n","    outputs = classifier_model(**test_dataset.encodings)\n","\n","# Get the predicted logits\n","logits = outputs.logits\n","\n","# Get the predicted probabilities\n","probs = torch.softmax(logits, dim=1)\n","\n","# Get the predicted labels (0 or 1)\n","y_pred = torch.argmax(probs, dim=1)"],"metadata":{"id":"cGCEGtcZrbMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","y_test = test_dataset.labels\n","\n","scores = classification_report(y_test, y_pred)\n","print(scores)"],"metadata":{"id":"SMWEsBOzuNXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example = 'Solar panels a clean source of energy.'\n","# example = 'A threat to humanity.'\n","# example = 'Solar panels are a dangerous threat to humanity.'\n","# example = \"This is a nonsense sentence.\"\n","# example = \"A goat went crazy and tripped on mushrooms.\"\n","\n","example_encoding = tokenizer([example], truncation=True, padding=True, return_tensors='pt')\n","\n","# Forward pass\n","with torch.no_grad():\n","    outputs = classifier_model(**example_encoding)\n","\n","# Get the predicted logits\n","logits = outputs.logits\n","\n","# Get the predicted probabilities\n","probs = torch.softmax(logits, dim=1)\n","\n","# Get the predicted labels (0 or 1)\n","y_pred = torch.argmax(probs, dim=1)\n","\n","result = 'Funny' if 'goat' in example else'Positive' if y_pred==1 else 'Negative'\n","print(result)"],"metadata":{"id":"0n_wl2tJrWgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Assignment: try to improve the model prediction performance\n","- Try running the above with a bigger dataset\n","- Try finetuning some of the hyperparameters\n","- Try out other models available on 🤗"],"metadata":{"id":"3PWQNk_-pt9j"}},{"cell_type":"code","source":[],"metadata":{"id":"skW-JCtq0nfc"},"execution_count":null,"outputs":[]}]}