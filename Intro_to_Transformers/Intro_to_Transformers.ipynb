{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14164,
     "status": "ok",
     "timestamp": 1687916600339,
     "user": {
      "displayName": "Ilja Rausch",
      "userId": "09224443308545937385"
     },
     "user_tz": 360
    },
    "id": "DeQcBJfw6RCq",
    "outputId": "ad43fa34-d861-4f39-ef7e-2a63b2e22b97",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:13.831798Z",
     "start_time": "2023-06-29T16:20:12.438679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.28.0)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (0.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (1.24.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (2023.6.3)\r\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (0.13.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.28.0) (4.65.0)\r\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.6.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.28.0) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.28.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.28.0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.28.0) (2023.5.7)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the transformers packages\n",
    "%pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vkE2U_qa51hV",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:17.773822Z",
     "start_time": "2023-06-29T16:20:13.831345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required modules:\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'  # You can specify the model size (e.g., 'gpt2', 'gpt2-medium', 'gpt2-large')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "726z2EEhcrxX",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:17.775734Z",
     "start_time": "2023-06-29T16:20:17.772328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Tokens: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n"
     ]
    }
   ],
   "source": [
    "# Let's have a closer look at tokenization\n",
    "ids = [t for t in range(0, 10)]\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(f'Ids: {ids}')\n",
    "print(f'Tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qyy423sdbsr2",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:17.785450Z",
     "start_time": "2023-06-29T16:20:17.775934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[31837,   594,   546,  6591,  2568,    13]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = 'Sentence about solar energy.'\n",
    "tokenizer.encode(input_sequence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T2FjM_p9b79X",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:17.791907Z",
     "start_time": "2023-06-29T16:20:17.787851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Sent', 'ence', 'Ġabout', 'Ġsolar', 'Ġenergy', '.']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([31837,   594,   546,  6591,  2568,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "64eYSSh66a5U",
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:22.583885Z",
     "start_time": "2023-06-29T16:20:17.793229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated Text 1: Sentence about solar energy.\n",
      "\n",
      "\n",
      "\n",
      "Generated Text 2: Sentence about solar energy.\n",
      "\n",
      "\"On a daily basis, we are being told that solar energy is an economic success story,\" Eriksson said in a press conference held after the hearing.\n",
      "\n",
      "The bill was approved in May, after\n",
      "\n",
      "\n",
      "\n",
      "Generated Text 3: Sentence about solar energy.\n",
      "\n",
      "\n",
      "\n",
      "Generated Text 4: Sentence about solar energy. Photo: AFP\n",
      "\n",
      "There are numerous ways that solar energy can be sold as energy. To those familiar with the topic, this makes the case for the importance of using solar to help keep you from dying. When one\n",
      "\n",
      "\n",
      "\n",
      "Generated Text 5: Sentence about solar energy. \"The government is already thinking about setting up an offshore company to do offshore solar generation.\n",
      "\n",
      "\"The government is aware of the possibility of such a move. But to give one example we will need to know about\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the input, i.e. your 'prompt'.\n",
    "input_sequence = 'Sentence about solar energy.'\n",
    "# input_sequence = 'This is a nonsense sentence.'\n",
    "# Encode the prompt\n",
    "encoded_input = tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "# Generate text using the trained GPT-2 model\n",
    "\n",
    "max_length = 50  # Maximum length of the generated text\n",
    "num_return_sequences = 5  # Number of text sequences to generate\n",
    "\n",
    "# https://huggingface.co/transformers/v4.4.2/main_classes/model.html#generation\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input,\n",
    "    max_length=max_length,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    temperature=1.0,  # Controls the randomness of the generated text (higher values make it more random)\n",
    "    repetition_penalty=1.0,  # Penalizes repeating words/phrases (higher values make it less likely to repeat)\n",
    "    do_sample=True,\n",
    "    top_k=50,  # Generates from the top k most likely next words\n",
    "    top_p=0.95,  # Generates from the cumulative probability until it reaches the given probability p\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "for i, sequence in enumerate(output_sequences):\n",
    "    print('\\n')\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    print(f\"Generated Text {i+1}: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aqdo5v21o6Bw",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-29T16:20:22.582178Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('GPT_training_pipeline.png') # Reference: https://youtu.be/bZQun8Y4L2A?t=76 (retrieved 06-04-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCM06uOvn9qY"
   },
   "source": [
    "# Fine-tune the GPT2 transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIu7qxw7tJt9"
   },
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h5jrtQkn6re",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling # https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/language_modeling.py\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = 'gpt2'  # You can specify the model size (e.g., 'gpt2', 'gpt2-medium', 'gpt2-large')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq7CfqyatNgc"
   },
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-SZl7mTmMNh",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_file_path = 'positive_statements.txt'\n",
    "\n",
    "# Load the text dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_file_path,\n",
    "    block_size=32  # Adjust the block size based on your dataset and memory capacity\n",
    ")\n",
    "\n",
    "# Split into train, eval and test sets\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.33, random_state=42)\n",
    "eval_set, test_set = train_test_split(test_set, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmcFwiVytVCE"
   },
   "source": [
    "## Fine-tune the model on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wQwSC1E6paj",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "output_dir = 'trained_models'  # Create the directory if it doesn't exist\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=64,  # Adjust the batch size based on your memory capacity\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,  # Adjust the learning rate as needed\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=eval_set,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(f'Elapsed time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMOLeRtcriBN",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model('trained_models/final_finetuned_gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9tKTWI0tEn4"
   },
   "source": [
    "## Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "occ7SiTEqo5Q",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_name = 'trained_models/final_finetuned_gpt2'\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(model_name, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DApF7LNM6Me",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "text = \"Sentence about solar energy.\"\n",
    "# text = \"This is a nonsense sentence.\"\n",
    "# text = \"A goat went crazy and tripped on mushrooms.\"\n",
    "text = \"The analyst shared stock market insights with the Financial Times.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the trained GPT-2 model\n",
    "max_length = 50  # Maximum length of the generated text\n",
    "num_return_sequences = 10  # Number of text sequences to generate\n",
    "\n",
    "# https://huggingface.co/transformers/v4.4.2/main_classes/model.html#generation\n",
    "output_sequences = finetuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    temperature=1.0,  # Controls the randomness of the generated text (higher values make it more random)\n",
    "    repetition_penalty=1.0,  # Penalizes repeating words/phrases (higher values make it less likely to repeat)\n",
    "    do_sample=True,\n",
    "    top_k=50,  # Generates from the top k most likely next words\n",
    "    top_p=0.95,  # Generates from the cumulative probability until it reaches the given probability p\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for output in output_sequences:\n",
    "  print('\\n')\n",
    "  # Decode the generated text\n",
    "  generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "  print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCil-6CWe0zZ"
   },
   "source": [
    "# Fine tuning into a Classifier\n",
    "GPT is a gnerative AI model, not designed for Classification tasks.\n",
    "However, there are many other models out there. Here, we will use ROBERTa.\n",
    "ROBERTa: https://huggingface.co/transformers/v4.4.2/model_doc/roberta.html\n",
    "\n",
    "Fine tuning tutorial: https://huggingface.co/transformers/v3.2.0/custom_datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUNy0_IxkWDW"
   },
   "source": [
    "## Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TblYUt48fUhP",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "positive_text = ''\n",
    "\n",
    "with open('positive_statements.txt', 'r') as pos_file:\n",
    "  positive_text = pos_file.read()\n",
    "\n",
    "positive_seqs = positive_text.split('\\n')\n",
    "positive_labels = [1]*len(positive_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGecchWbf8VY",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "negative_text = ''\n",
    "\n",
    "with open('negative_statements.txt', 'r') as neg_file:\n",
    "  negative_text = neg_file.read()\n",
    "\n",
    "negative_seqs = negative_text.split('\\n')\n",
    "negative_labels = [0]*len(negative_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7j-MH1lgQqv",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = np.array(positive_seqs + negative_seqs)\n",
    "texts = np.array([text.replace(':', '') for text in texts])\n",
    "labels = np.array(positive_labels + negative_labels)\n",
    "\n",
    "# Select only a small part of the data to reduce training time!\n",
    "ids_to_select = np.array(range(0, len(labels)))\n",
    "np.random.shuffle(ids_to_select)\n",
    "ids_to_select = ids_to_select[:400]\n",
    "texts = texts[ids_to_select]\n",
    "labels = labels[ids_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBop8vI7gz0w",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.33, random_state=35858)\n",
    "\n",
    "eval_texts, test_texts, eval_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.1, random_state=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76BcEVjfiZ6p",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "  sentiment = 'positive' if train_labels[i] else 'negative'\n",
    "  print(f'Sentence: {train_texts[i]} Sentiment: {sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB9yJXJjmnjE",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class SolarDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBVN2TfZkZuw"
   },
   "source": [
    "## Tokenize using ROBERTa tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IpA4LV4gY63",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import SqueezeBertTokenizerFast\n",
    "tokenizer = SqueezeBertTokenizerFast.from_pretrained('squeezebert/squeezebert-uncased')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Compute the tokens\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "eval_encodings = tokenizer(list(eval_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = SolarDataset(train_encodings, train_labels)\n",
    "eval_dataset = SolarDataset(eval_encodings, eval_labels)\n",
    "test_dataset = SolarDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MI1-498kg5o"
   },
   "source": [
    "## Load a pretrained ROBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc1DqVbSi1vO",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import SqueezeBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./classification_results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    logging_dir='./classification_logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Following the 🤗 recommendation for the choice of the pretrained model:\n",
    "# https://huggingface.co/transformers/v4.4.2/model_doc/squeezebert.html#overview\n",
    "classifier_model = SqueezeBertForSequenceClassification.from_pretrained(\"squeezebert/squeezebert-mnli-headless\")\n",
    "\n",
    "classifier_trainer = Trainer(\n",
    "    model=classifier_model,              # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQWR0dFwlShu",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "classifier_trainer.save_model('trained_models/final_finetuned_squeezebert_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sc4KbmbXow-6",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_name = 'trained_models/final_finetuned_squeezebert_classifier'\n",
    "classifier_model = SqueezeBertForSequenceClassification.from_pretrained(model_name, local_files_only=True)\n",
    "classifier_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGCEGtcZrbMj",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = classifier_model(**test_dataset.encodings)\n",
    "\n",
    "# Get the predicted logits\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted probabilities\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Get the predicted labels (0 or 1)\n",
    "y_pred = torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMWEsBOzuNXz",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = test_dataset.labels\n",
    "\n",
    "scores = classification_report(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n_wl2tJrWgT",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "example = 'Solar panels a clean source of energy.'\n",
    "# example = 'A threat to humanity.'\n",
    "# example = 'Solar panels are a dangerous threat to humanity.'\n",
    "# example = \"This is a nonsense sentence.\"\n",
    "# example = \"A goat went crazy and tripped on mushrooms.\"\n",
    "\n",
    "example_encoding = tokenizer([example], truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = classifier_model(**example_encoding)\n",
    "\n",
    "# Get the predicted logits\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted probabilities\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Get the predicted labels (0 or 1)\n",
    "y_pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "result = 'Funny' if 'goat' in example else'Positive' if y_pred==1 else 'Negative'\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PWQNk_-pt9j"
   },
   "source": [
    "# Assignment: try to improve the model prediction performance\n",
    "- Try running the above with a bigger dataset\n",
    "- Try finetuning some of the hyperparameters\n",
    "- Try out other models available on 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skW-JCtq0nfc",
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoBgMrtJBJf7Gz5f9M60UF",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
